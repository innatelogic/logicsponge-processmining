{
  "nn": {
    "lr": 0.001,
    "batch_size": 8,
    "epochs": 20
  },
  "rl": {
    "lr": 0.001,
    "batch_size": 64,
    "epochs": 20,
    "gamma": 0.99
  },
  "lstm": {
    "vocab_size": 24,
    "embedding_dim": 24,
    "hidden_dim": 512,
    "output_dim": 24
  },
  "transformer": {
    "vocab_size": 24,
    "embedding_dim": 24,
    "hidden_dim": 512,
    "output_dim": 24
  },
  "qlearning": {
    "vocab_size": 24,
    "embedding_dim": 24,
    "hidden_dim": 512,
    "output_dim": 24
  }
}